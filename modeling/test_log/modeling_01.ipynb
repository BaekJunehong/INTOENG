{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. 기본환경 세팅"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "gpu 환경 체크"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU 0: NVIDIA GeForce RTX 3050 Ti Laptop GPU (UUID: GPU-b5b4dfdb-dd9f-1ff3-bc8b-d74672661a90)\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi -L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(device)\n",
    "\n",
    "# output : \"cuda\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 데이터셋과 transformers 설치\n",
    "# %pip install datasets>=2.6.1\n",
    "# %pip install git+https://github.com/huggingface/transformers\n",
    "\n",
    "# # 오디오 처리를 위한 librosa 설치\n",
    "# %pip install librosa\n",
    "\n",
    "# # 성능 측정을 위한 evaluate와 jiwer 설치\n",
    "# %pip install evaluate>=0.30\n",
    "# %pip install jiwer\n",
    "\n",
    "# # 인터랙티브 인터페이스를 위한 gradio 설치\n",
    "# %pip install gradio\n",
    "\n",
    "# # Transformers와 PyTorch를 함께 사용하기 위한 accelerate 설치\n",
    "# %pip install transformers[torch]\n",
    "# %pip install accelerate>=0.20.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\juneh\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Dict, List, Union\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 함수 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class DataCollatorSpeechSeq2SeqWithPadding:\n",
    "    \"\"\"\n",
    "    Use Data Collator to perform Speech Seq2Seq with padding\n",
    "    \"\"\"\n",
    "    processor: Any\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        # split inputs and labels since they have to be of different lengths and need different padding methods\n",
    "        # first treat the audio inputs by simply returning torch tensors\n",
    "        input_features = [{\"input_features\": feature[\"input_features\"]} for feature in features]\n",
    "        batch = self.processor.feature_extractor.pad(input_features, return_tensors=\"pt\")\n",
    "\n",
    "        # get the tokenized label sequences\n",
    "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
    "        # pad the labels to max length\n",
    "        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors=\"pt\")\n",
    "\n",
    "        # replace padding with -100 to ignore loss correctly\n",
    "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
    "\n",
    "        # if bos token is appended in previous tokenization step,\n",
    "        # cut bos token here as it's append later anyways\n",
    "        if (labels[:, 0] == self.processor.tokenizer.bos_token_id).all().cpu().item():\n",
    "            labels = labels[:, 1:]\n",
    "\n",
    "        batch[\"labels\"] = labels\n",
    "\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\juneh\\AppData\\Roaming\\Python\\Python37\\site-packages\\ipykernel_launcher.py:3: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_metric\n",
    "\n",
    "wer_metric = load_metric(\"wer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(pred):\n",
    "    labels_ids = pred.label_ids\n",
    "    pred_ids = pred.predictions\n",
    "\n",
    "    pred_str = processor.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "    labels_ids[labels_ids == -100] = processor.tokenizer.pad_token_id\n",
    "    label_str = processor.batch_decode(labels_ids, skip_special_tokens=True)\n",
    "\n",
    "    wer = wer_metric.compute(predictions=pred_str, references=label_str)\n",
    "\n",
    "    return {\"wer\": wer}\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 음성 처리에 필요한 기능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import WhisperFeatureExtractor, WhisperTokenizer, WhisperProcessor\n",
    "\n",
    "# Load Feature extractor: WhisperFeatureExtractor\n",
    "feature_extractor = WhisperFeatureExtractor.from_pretrained(\"openai/whisper-base\")\n",
    "\n",
    "# Load Tokenizer: WhisperTokenizer\n",
    "tokenizer = WhisperTokenizer.from_pretrained(\"openai/whisper-base\", language=\"english\", task=\"transcribe\")\n",
    "\n",
    "# Load Processor: WhisperProcessor\n",
    "processor = WhisperProcessor.from_pretrained(\"openai/whisper-base\", language=\"english\", task=\"transcribe\")\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 데이터셋 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "\n",
    "# Load the combined 'train' dataset\n",
    "loaded_train_dataset = load_from_disk('./data/map_dataset/train')\n",
    "\n",
    "# Load the combined 'test' dataset\n",
    "loaded_test_dataset = load_from_disk('./data/map_dataset/test')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['input_features', 'labels'],\n",
      "    num_rows: 9523\n",
      "})\n",
      "Dataset({\n",
      "    features: ['input_features', 'labels'],\n",
      "    num_rows: 502\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(loaded_train_dataset)\n",
    "print(loaded_test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. 학습 및 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\juneh\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\transformers\\optimization.py:415: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training 시작\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1000 [00:00<?, ?it/s]`use_cache = True` is incompatible with gradient checkpointing. Setting `use_cache = False`...\n",
      "  6%|▋         | 63/1000 [4:30:01<67:03:19, 257.63s/it]"
     ]
    }
   ],
   "source": [
    "# STEP 5.1. Initialize the Data collator\n",
    "data_collator = DataCollatorSpeechSeq2SeqWithPadding(processor=processor)\n",
    "\n",
    "# STEP 5.1. Define evaluation metric\n",
    "import evaluate\n",
    "metric = evaluate.load(\"wer\")\n",
    "\n",
    "# STEP 5.3. Load a pre-trained Checkpoint\n",
    "from transformers import WhisperForConditionalGeneration\n",
    "model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-base\")\n",
    "\n",
    "model.config.forced_decoder_ids = None\n",
    "model.config.suppress_tokens = []\n",
    "     \n",
    "\n",
    "# STEP 5.4. Define the training configuration\n",
    "\"\"\"\n",
    "Check for Seq2SeqTrainingArguments here:\n",
    "https://huggingface.co/docs/transformers/main_classes/trainer#transformers.Seq2SeqTrainingArguments\n",
    "\"\"\"\n",
    "from transformers import Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./whisper_base_0401\",  # 저장된 모델 및 결과물의 디렉토리 경로\n",
    "    per_device_train_batch_size=32,  # 한 번에 처리되는 훈련 배치 크기\n",
    "    gradient_accumulation_steps=2,  # 배치 크기 감소시 그래디언트 누적을 통한 학습 안정화\n",
    "    learning_rate=1e-5,  # 학습률\n",
    "    warmup_steps=100,  # 초기 학습률 조정을 위한 웜업 스텝 수 / 일반적으로는 10% ~ 20%의 전체 학습 스텝 수에 해당하는 값을 시도\n",
    "    max_steps=1000,  # 전체 훈련 스텝 수\n",
    "    gradient_checkpointing=True,  # 그래디언트 체크포인팅을 통한 메모리 절약\n",
    "    # fp16=True,  # FP16 형식으로 훈련 수행 (반정밀도 부동소수점)( cpu 가동시 안씀)\n",
    "    evaluation_strategy=\"no\",  # 검증 수행 전략 설정\n",
    "    per_device_eval_batch_size=16,  # 한 번에 처리되는 검증 배치 크기\n",
    "    predict_with_generate=True,  # 생성된 토큰을 통해 예측 수행\n",
    "    generation_max_length=225,  # 생성된 토큰의 최대 길이 (225유지)\n",
    "    eval_steps=100,  # 검증 수행 스텝 수\n",
    "    logging_steps=100,  # 로그 기록 스텝 수\n",
    "    load_best_model_at_end=False,  # 훈련 종료 시 최적 모델 로드 여부\n",
    "    metric_for_best_model=\"wer\",  # 최적 모델 선정을 위한 평가 지표 \n",
    "    greater_is_better=False,  # 평가 지표 값이 높을수록 좋은지 여부\n",
    "    save_steps=200  # 변경된 save_steps 값\n",
    ")\n",
    "\n",
    "# Initialize a trainer.\n",
    "\"\"\"\n",
    "Forward the training arguments to the Hugging Face trainer along with our model,\n",
    "dataset, data collator and compute_metrics function.\n",
    "\"\"\"\n",
    "# 지정된 인자 및 구성요소로 트레이너를 초기화합니다\n",
    "trainer = Seq2SeqTrainer(\n",
    "    args=training_args,                   # 이전에 정의한 훈련 인자\n",
    "    model=model,                          # 훈련할 ASR 모델\n",
    "    train_dataset=loaded_train_dataset,# 훈련 데이터셋\n",
    "    eval_dataset=loaded_test_dataset,  # 평가 데이터셋\n",
    "    data_collator=data_collator,           # 데이터 전처리를 위한 데이터 콜레이터\n",
    "    compute_metrics=compute_metrics,          # wer 메트릭을 계산하는 함수\n",
    "    tokenizer=processor.feature_extractor, # 입력 오디오 데이터를 처리하기 위한 토크나이저\n",
    ")\n",
    "\n",
    "# Save processor object before starting training\n",
    "processor.save_pretrained(training_args.output_dir)\n",
    "\n",
    "# STEP 5.5. Training\n",
    "\"\"\"\n",
    "Training will take appr. 5-10 hours depending on your GPU.\n",
    "\"\"\"\n",
    "print('Training 시작')\n",
    "trainer.train()   # <-- training 시작\n",
    "print('Training 완료')\n",
    "\n",
    "#\"Step\": 모델의 훈련 과정에서 진행되는 각 스텝을 나타내는 숫자입니다.\n",
    "#스텝은 주로 배치(batch) 단위로 모델이 업데이트되는 지점을 의미합니다.\n",
    "\n",
    "# Training Loss는 모델이 훈련 데이터에 대해 얼마나 정확하게 예측하는지를 나타내는 지표\n",
    "# Training Loss가 감소하면 모델이 훈련 데이터에 대해 더 잘 학습하고 있는 것\n",
    "# 모델이 데이터에 더 잘 적합되고 있다는 것을 의미\n",
    "\n",
    "# Validation Loss는 모델이 이전에 본 적이 없는 검증 데이터에 대한 예측 정확도\n",
    "# 훈련 과정 중에 일정 주기마다 검증 데이터를 사용하여 Validation Loss를 계산\n",
    "# 이 값이 감소하면 모델이 일반화되고 있는 것을 의미\n",
    "# 모델이 훈련 데이터뿐만 아니라 새로운 데이터에도 잘 예측할 수 있도록 학습되고 있다는 것\n",
    "\n",
    "# \"CER\" (Character Error Rate): 훈련 중에 일정 주기마다 검증 데이터를 사용하여\n",
    "#모델의 문자 에러 비율(CER)을 평가한 값입니다.\n",
    "#CER은 텍스트 분야에서 자주 사용되는 평가 지표 중 하나로,\n",
    "#모델이 생성한 텍스트와 실제 텍스트 사이의 문자 수준 오류 비율을 나타냅니다.\n",
    "#CER이 낮을수록 모델의 성능이 좋다고 판단됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "학습된 모델 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. 모델 저장"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "trainer.save_model(\"./model/whisper_base_0331\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Load the trained tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"/content/whisper_base_0824_ver1\")\n",
    "\n",
    "# Specify the directory where you want to save the tokenizer\n",
    "save_directory = \"/content/drive/MyDrive/model/whisper_base_0824\"\n",
    "\n",
    "# Save the tokenizer to the specified directory\n",
    "tokenizer.save_pretrained(save_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. 모델 실행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "import soundfile as sf  # soundfile 라이브러리 사용\n",
    "\n",
    "# ASR 파이프라인 초기화\n",
    "model_name_or_path = \"/content/drive/MyDrive/model/whisper_base_0824\"\n",
    "asr = pipeline(model=model_name_or_path, task=\"automatic-speech-recognition\")\n",
    "\n",
    "# 음성 파일 경로 리스트\n",
    "audio_file_paths = [\n",
    "    \"/content/drive/MyDrive/data_file/평가 데이터셋/K00013886-BFG23-L1N2D2-E-K0KK-02601769.wav\",\n",
    "    \"/content/drive/MyDrive/data_file/평가 데이터셋/K00013886-BFG23-L1N2D2-E-K0KK-02743434.wav\",\n",
    "    \"/content/drive/MyDrive/data_file/평가 데이터셋/K00013886-BFG23-L1N2D2-E-K0KK-02989139.wav\",\n",
    "    \"/content/drive/MyDrive/data_file/평가 데이터셋/K00014982-BFG20-L1N2D1-E-K0KK-03006747.wav\",\n",
    "    \"/content/drive/MyDrive/data_file/평가 데이터셋/K00014982-BFG20-L1N2D1-E-K0KK-03017978.wav\",\n",
    "    \"/content/drive/MyDrive/data_file/평가 데이터셋/K00014982-BFG20-L1N2D4-E-K0KK-02872759.wav\",\n",
    "    # 추가 음성 파일 경로\n",
    "]\n",
    "\n",
    "# ASR 함수 정의\n",
    "def transcribe_audio(audio_path):\n",
    "    transcription = asr(audio_path)\n",
    "    return transcription['text']  # Use 'text' key to get the transcribed text\n",
    "\n",
    "# 각 음성 파일에 대한 처리 및 출력\n",
    "for audio_file_path in audio_file_paths:\n",
    "    transcription_text = transcribe_audio(audio_file_path)\n",
    "    print(f\"음성 파일: {audio_file_path}\")\n",
    "    print(\"텍스트 출력:\", transcription_text)\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
